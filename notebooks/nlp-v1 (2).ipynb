{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13950011,"sourceType":"datasetVersion","datasetId":8891520}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download fr_core_news_sm\n!python -m spacy download en_core_web_sm\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:40:56.599505Z","iopub.execute_input":"2025-12-02T18:40:56.599823Z","iopub.status.idle":"2025-12-02T18:41:28.180315Z","shell.execute_reply.started":"2025-12-02T18:40:56.599799Z","shell.execute_reply":"2025-12-02T18:41:28.179490Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.12.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2025.3.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2022.3.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.19.0->spacy) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nCollecting numpy>=1.19.0 (from spacy)\n  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nRequirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2025.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.19.0->spacy) (2022.3.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.19.0->spacy) (2024.2.0)\nINFO: pip is looking at multiple versions of mkl-fft to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_fft (from numpy>=1.19.0->spacy)\n  Downloading mkl_fft-2.1.1-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (7.3 kB)\nINFO: pip is looking at multiple versions of mkl-random to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_random (from numpy>=1.19.0->spacy)\n  Downloading mkl_random-1.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nINFO: pip is looking at multiple versions of mkl-umath to determine which version is compatible with other requirements. This could take a while.\nCollecting mkl_umath (from numpy>=1.19.0->spacy)\n  Downloading mkl_umath-0.3.0-0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.3 kB)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.19.0->spacy) (2024.2.0)\nDownloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.5\nCollecting fr-core-news-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m75.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fr-core-news-sm\nSuccessfully installed fr-core-news-sm-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\nCollecting en-core-web-sm==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install spacy[transformers]\n!python -m spacy download en_core_web_trf\n!python -m spacy download fr_core_news_trf\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:41:28.181788Z","iopub.execute_input":"2025-12-02T18:41:28.182126Z","iopub.status.idle":"2025-12-02T18:43:20.906810Z","shell.execute_reply.started":"2025-12-02T18:41:28.182078Z","shell.execute_reply":"2025-12-02T18:43:20.905938Z"},"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy[transformers] in /usr/local/lib/python3.11/dist-packages (3.8.7)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (0.16.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.3.5)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (2.12.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy[transformers]) (3.5.0)\nCollecting spacy_transformers<1.4.0,>=1.1.2 (from spacy[transformers])\n  Downloading spacy_transformers-1.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy[transformers]) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[transformers]) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[transformers]) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[transformers]) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy[transformers]) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]) (2025.10.5)\nCollecting transformers<4.50.0,>=3.4.0 (from spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (2.6.0+cu124)\nCollecting spacy-alignments<1.0.0,>=0.7.2 (from spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading spacy_alignments-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[transformers]) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy[transformers]) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[transformers]) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[transformers]) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy[transformers]) (14.2.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[transformers]) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy[transformers]) (7.3.0.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy[transformers]) (3.0.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy[transformers]) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[transformers]) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[transformers]) (2.19.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy[transformers]) (1.17.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (3.20.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (3.5)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers])\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (0.36.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (2025.11.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (0.5.3)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers<4.50.0,>=3.4.0->spacy_transformers<1.4.0,>=1.1.2->spacy[transformers]) (1.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy[transformers]) (0.1.2)\nDownloading spacy_transformers-1.3.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (758 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m758.8/758.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading spacy_alignments-0.9.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.2/314.2 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: spacy-alignments, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, transformers, spacy_transformers\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.53.3\n    Uninstalling transformers-4.53.3:\n      Successfully uninstalled transformers-4.53.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 spacy-alignments-0.9.2 spacy_transformers-1.3.9 transformers-4.49.0\nCollecting en-core-web-trf==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.8.0/en_core_web_trf-3.8.0-py3-none-any.whl (457.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.4/457.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting spacy-curated-transformers<1.0.0,>=0.2.2 (from en-core-web-trf==3.8.0)\n  Downloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl.metadata (2.7 kB)\nCollecting curated-transformers<0.2.0,>=0.1.0 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n  Downloading curated_transformers-0.1.1-py2.py3-none-any.whl.metadata (965 bytes)\nCollecting curated-tokenizers<0.1.0,>=0.0.9 (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0)\n  Downloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\nRequirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.6.0+cu124)\nRequirement already satisfied: regex>=2022 in /usr/local/lib/python3.11/dist-packages (from curated-tokenizers<0.1.0,>=0.0.9->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.11.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (4.15.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2025.10.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.12.0->spacy-curated-transformers<1.0.0,>=0.2.2->en-core-web-trf==3.8.0) (3.0.3)\nDownloading spacy_curated_transformers-0.3.1-py2.py3-none-any.whl (237 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m237.9/237.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading curated_tokenizers-0.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (735 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m735.6/735.6 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading curated_transformers-0.1.1-py2.py3-none-any.whl (25 kB)\nInstalling collected packages: curated-tokenizers, curated-transformers, spacy-curated-transformers, en-core-web-trf\nSuccessfully installed curated-tokenizers-0.0.9 curated-transformers-0.1.1 en-core-web-trf-3.8.0 spacy-curated-transformers-0.3.1\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_trf')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n\n\u001b[38;5;1m✘ No compatible package found for 'fr_core_news_trf' (spaCy v3.8.7)\u001b[0m\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Fix torch import error: \"AttributeError torch.has no attribute 'types'\"\n!pip uninstall -y torch torchvision torchaudio\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:43:20.907992Z","iopub.execute_input":"2025-12-02T18:43:20.908387Z","iopub.status.idle":"2025-12-02T18:46:09.991610Z","shell.execute_reply.started":"2025-12-02T18:43:20.908358Z","shell.execute_reply":"2025-12-02T18:46:09.990443Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: torch 2.6.0+cu124\nUninstalling torch-2.6.0+cu124:\n  Successfully uninstalled torch-2.6.0+cu124\nFound existing installation: torchvision 0.21.0+cu124\nUninstalling torchvision-0.21.0+cu124:\n  Successfully uninstalled torchvision-0.21.0+cu124\nFound existing installation: torchaudio 2.6.0+cu124\nUninstalling torchaudio-2.6.0+cu124:\n  Successfully uninstalled torchaudio-2.6.0+cu124\nLooking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\nCollecting torchvision\n  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\nCollecting torchaudio\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.15.0)\nCollecting sympy>=1.13.3 (from torch)\n  Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.10.0)\nCollecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m20.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting triton==3.3.1 (from torch)\n  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.3.5)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.3.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.3)\nDownloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (905.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m905.3/905.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: sympy\n    Found existing installation: sympy 1.13.1\n    Uninstalling sympy-1.13.1:\n      Successfully uninstalled sympy-1.13.1\nSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.14.0 torch-2.7.1+cu118 torchaudio-2.7.1+cu118 torchvision-0.22.1+cu118 triton-3.3.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nprint(\"Torch version:\", torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:46:09.993691Z","iopub.execute_input":"2025-12-02T18:46:09.993958Z","iopub.status.idle":"2025-12-02T18:46:12.266999Z","shell.execute_reply.started":"2025-12-02T18:46:09.993926Z","shell.execute_reply":"2025-12-02T18:46:12.266273Z"}},"outputs":[{"name":"stdout","text":"Torch version: 2.7.1+cu118\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install protobuf==4.23.4\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:46:12.267720Z","iopub.execute_input":"2025-12-02T18:46:12.268142Z","iopub.status.idle":"2025-12-02T18:46:16.971648Z","shell.execute_reply.started":"2025-12-02T18:46:12.268096Z","shell.execute_reply":"2025-12-02T18:46:16.970711Z"}},"outputs":[{"name":"stdout","text":"Collecting protobuf==4.23.4\n  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\nDownloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 6.33.0\n    Uninstalling protobuf-6.33.0:\n      Successfully uninstalled protobuf-6.33.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.23.4 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 4.23.4 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.23.4 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 4.23.4 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.23.4 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.23.4 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-4.23.4\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install numpy==1.26.4 --force-reinstall\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:46:16.972833Z","iopub.execute_input":"2025-12-02T18:46:16.973226Z","iopub.status.idle":"2025-12-02T18:46:24.064294Z","shell.execute_reply.started":"2025-12-02T18:46:16.973193Z","shell.execute_reply":"2025-12-02T18:46:24.063488Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.26.4\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.5\n    Uninstalling numpy-2.3.5:\n      Successfully uninstalled numpy-2.3.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 4.23.4 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.23.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"pip install --upgrade numpy scikit-learn\npip install --upgrade nltk\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:46:24.065388Z","iopub.execute_input":"2025-12-02T18:46:24.065735Z","iopub.status.idle":"2025-12-02T18:46:24.073201Z","shell.execute_reply.started":"2025-12-02T18:46:24.065705Z","shell.execute_reply":"2025-12-02T18:46:24.070769Z"}},"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_47/4139023756.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    pip install --upgrade numpy scikit-learn\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"],"ename":"SyntaxError","evalue":"invalid syntax (4139023756.py, line 1)","output_type":"error"}],"execution_count":7},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download fr_core_news_md\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:52:43.647726Z","iopub.execute_input":"2025-12-02T18:52:43.648015Z","iopub.status.idle":"2025-12-02T18:52:59.717484Z","shell.execute_reply.started":"2025-12-02T18:52:43.647993Z","shell.execute_reply":"2025-12-02T18:52:59.716564Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\nRequirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.12.4)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\nRequirement already satisfied: pydantic-core==2.41.5 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\nRequirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\nRequirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\nRequirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\nCollecting numpy>=1.19.0 (from spacy)\n  Using cached numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.3)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\nUsing cached numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 4.23.4 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.23.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.5\nCollecting fr-core-news-md==3.8.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_md-3.8.0/fr_core_news_md-3.8.0-py3-none-any.whl (45.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: fr-core-news-md\nSuccessfully installed fr-core-news-md-3.8.0\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('fr_core_news_md')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T18:53:09.751Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['text'].head(10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:04.649436Z","iopub.execute_input":"2025-12-02T18:59:04.649691Z","iopub.status.idle":"2025-12-02T18:59:04.655970Z","shell.execute_reply.started":"2025-12-02T18:59:04.649673Z","shell.execute_reply":"2025-12-02T18:59:04.655219Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"0    I have never been more happy on this trip beca...\n1    You can't go wrong with the Jazz Festival in y...\n2    Je viens de passer quelques jours à Sidi Bou S...\n3    It's just so cool when you get to see the worl...\n4    Je reviens d'un séjour à Carthage et je voulai...\n5    The only thing that surprised me is how well t...\n6    Je dois être honnête sur mon expérience à Sidi...\n7    It's cool to know what you have around here is...\n8    Mon voyage à Hammamet a été absolument merveil...\n9    I just don't think this is enough for me at th...\nName: text, dtype: object"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# Nombre de lieux uniques\nnombre_places_uniques = df['place'].nunique()\nprint(f\"🌍 Nombre de lieux uniques dans le dataset : {nombre_places_uniques}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:07.554773Z","iopub.execute_input":"2025-12-02T18:59:07.555786Z","iopub.status.idle":"2025-12-02T18:59:07.561794Z","shell.execute_reply.started":"2025-12-02T18:59:07.555740Z","shell.execute_reply":"2025-12-02T18:59:07.560944Z"}},"outputs":[{"name":"stdout","text":"🌍 Nombre de lieux uniques dans le dataset : 5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Nombre total de lignes (commentaires)\nnombre_lignes = len(df)\nprint(f\"📝 Nombre total de lignes dans le dataset : {nombre_lignes}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:10.416810Z","iopub.execute_input":"2025-12-02T18:59:10.417686Z","iopub.status.idle":"2025-12-02T18:59:10.422281Z","shell.execute_reply.started":"2025-12-02T18:59:10.417651Z","shell.execute_reply":"2025-12-02T18:59:10.421491Z"}},"outputs":[{"name":"stdout","text":"📝 Nombre total de lignes dans le dataset : 10000\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install \"numpy<2\" --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:57:30.750986Z","iopub.execute_input":"2025-12-02T18:57:30.751861Z","iopub.status.idle":"2025-12-02T18:57:37.270127Z","shell.execute_reply.started":"2025-12-02T18:57:30.751828Z","shell.execute_reply":"2025-12-02T18:57:37.269319Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy<2\n  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.5\n    Uninstalling numpy-2.3.5:\n      Successfully uninstalled numpy-2.3.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 4.23.4 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.23.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-12-02T18:58:21.742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport os\n\n# Path du fichier exactement comme tu l'as écrit\npath = \"/kaggle/input/tunisia-sensory-data/tunisia_natural_comments_10000_20251201_194300.csv\"\n\n# Vérification\nprint(\"Fichier existe :\", os.path.exists(path))\n\n# Lecture du CSV\ndf = pd.read_csv(path)\n\n# Afficher les colonnes\nprint(\"\\n📌 Colonnes du dataset :\")\nprint(df.columns)\n\n# Aperçu des 5 premières lignes\nprint(\"\\n📌 Aperçu du dataset :\")\ndf.head()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:58:56.925794Z","iopub.execute_input":"2025-12-02T18:58:56.926098Z","iopub.status.idle":"2025-12-02T18:58:57.355764Z","shell.execute_reply.started":"2025-12-02T18:58:56.926074Z","shell.execute_reply":"2025-12-02T18:58:57.355041Z"}},"outputs":[{"name":"stdout","text":"Fichier existe : True\n\n📌 Colonnes du dataset :\nIndex(['id', 'text', 'language', 'place', 'place_type', 'target_sentiment',\n       'predicted_sentiment', 'sentiment_confidence', 'sentiment_match',\n       'rating', 'timestamp', 'word_count'],\n      dtype='object')\n\n📌 Aperçu du dataset :\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"           id                                               text language  \\\n0  TUN_000000  I have never been more happy on this trip beca...       en   \n1  TUN_000001  You can't go wrong with the Jazz Festival in y...       en   \n2  TUN_000002  Je viens de passer quelques jours à Sidi Bou S...       fr   \n3  TUN_000003  It's just so cool when you get to see the worl...       en   \n4  TUN_000004  Je reviens d'un séjour à Carthage et je voulai...       fr   \n\n           place           place_type target_sentiment predicted_sentiment  \\\n0         Djerba               island         positive            positive   \n1         Djerba               island         positive             neutral   \n2  Sidi Bou Said      coastal village         positive             neutral   \n3   Tunis Medina        historic city         negative             neutral   \n4       Carthage  archaeological site         positive             neutral   \n\n   sentiment_confidence  sentiment_match  rating            timestamp  \\\n0              0.766667             True       4  2025-12-01 16:43:22   \n1              0.500000            False       3  2025-12-01 16:43:23   \n2              0.500000            False       3  2025-12-01 16:43:24   \n3              0.500000            False       4  2025-12-01 16:43:25   \n4              0.500000            False       4  2025-12-01 16:43:26   \n\n   word_count  \n0          16  \n1          45  \n2          63  \n3          62  \n4          69  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>language</th>\n      <th>place</th>\n      <th>place_type</th>\n      <th>target_sentiment</th>\n      <th>predicted_sentiment</th>\n      <th>sentiment_confidence</th>\n      <th>sentiment_match</th>\n      <th>rating</th>\n      <th>timestamp</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TUN_000000</td>\n      <td>I have never been more happy on this trip beca...</td>\n      <td>en</td>\n      <td>Djerba</td>\n      <td>island</td>\n      <td>positive</td>\n      <td>positive</td>\n      <td>0.766667</td>\n      <td>True</td>\n      <td>4</td>\n      <td>2025-12-01 16:43:22</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TUN_000001</td>\n      <td>You can't go wrong with the Jazz Festival in y...</td>\n      <td>en</td>\n      <td>Djerba</td>\n      <td>island</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.500000</td>\n      <td>False</td>\n      <td>3</td>\n      <td>2025-12-01 16:43:23</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TUN_000002</td>\n      <td>Je viens de passer quelques jours à Sidi Bou S...</td>\n      <td>fr</td>\n      <td>Sidi Bou Said</td>\n      <td>coastal village</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.500000</td>\n      <td>False</td>\n      <td>3</td>\n      <td>2025-12-01 16:43:24</td>\n      <td>63</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TUN_000003</td>\n      <td>It's just so cool when you get to see the worl...</td>\n      <td>en</td>\n      <td>Tunis Medina</td>\n      <td>historic city</td>\n      <td>negative</td>\n      <td>neutral</td>\n      <td>0.500000</td>\n      <td>False</td>\n      <td>4</td>\n      <td>2025-12-01 16:43:25</td>\n      <td>62</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TUN_000004</td>\n      <td>Je reviens d'un séjour à Carthage et je voulai...</td>\n      <td>fr</td>\n      <td>Carthage</td>\n      <td>archaeological site</td>\n      <td>positive</td>\n      <td>neutral</td>\n      <td>0.500000</td>\n      <td>False</td>\n      <td>4</td>\n      <td>2025-12-01 16:43:26</td>\n      <td>69</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport tensorflow as tf\n\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"Torch CUDA version:\", torch.version.cuda)\nprint(\"TensorFlow version:\", tf.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:15.114838Z","iopub.execute_input":"2025-12-02T18:59:15.115585Z","iopub.status.idle":"2025-12-02T18:59:22.781173Z","shell.execute_reply.started":"2025-12-02T18:59:15.115557Z","shell.execute_reply":"2025-12-02T18:59:22.780346Z"}},"outputs":[{"name":"stderr","text":"2025-12-02 18:59:17.092353: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764701957.116231     280 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764701957.123297     280 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"PyTorch version: 2.7.1+cu118\nTorch CUDA version: 11.8\nTensorFlow version: 2.18.0\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom transformers import pipeline\nfrom sentence_transformers import SentenceTransformer\nfrom collections import Counter\nimport json\nimport re\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Bibliothèques NLP\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer, WordNetLemmatizer\nfrom nltk.stem.snowball import FrenchStemmer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:38.395267Z","iopub.execute_input":"2025-12-02T18:59:38.396195Z","iopub.status.idle":"2025-12-02T18:59:44.891137Z","shell.execute_reply.started":"2025-12-02T18:59:38.396166Z","shell.execute_reply":"2025-12-02T18:59:44.890447Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ---------------------------\n# 📌 DEVICE\n# ---------------------------\ndevice = 0 if torch.cuda.is_available() else -1\nprint(f\"📱 Device: {'GPU ✅' if device == 0 else 'CPU'}\")\n\n# ---------------------------\n# 📌 NLTK Ressources\n# ---------------------------\nimport nltk\nfor resource in ['punkt', 'stopwords', 'wordnet']:\n    try:\n        nltk.data.find(\n            f'tokenizers/{resource}' if resource == 'punkt' else f'corpora/{resource}'\n        )\n    except LookupError:\n        nltk.download(resource, quiet=True)\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\n# Lemmatizer EN\nlemmatizer_en = WordNetLemmatizer()\n\n# Stemming EN (si tu gardes)\nstemmer_en = SnowballStemmer(\"english\")\n\n# Stopwords\nstop_words_fr = set(stopwords.words(\"french\"))\nstop_words_en = set(stopwords.words(\"english\"))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:59:53.527620Z","iopub.execute_input":"2025-12-02T18:59:53.529005Z","iopub.status.idle":"2025-12-02T19:00:01.160316Z","shell.execute_reply.started":"2025-12-02T18:59:53.528965Z","shell.execute_reply":"2025-12-02T19:00:01.159637Z"}},"outputs":[{"name":"stdout","text":"📱 Device: GPU ✅\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"sentiment_words_to_keep = {\n    # Positifs français\n    'bon', 'bonne', 'super', 'top', 'magnifique', 'incroyable', 'parfait',\n    'excellent', 'génial', 'formidable', 'extraordinaire', 'merveilleux',\n    'splendide', 'sublime', 'fantastique', 'exceptionnel', 'impressionnant',\n    'délicieux', 'savoureux', 'agréable', 'charmant', 'pittoresque',\n    'beau', 'belle', 'superbe', 'remarquable', 'admirable',\n    \n    # Positifs anglais\n    'great', 'good', 'nice', 'amazing', 'beautiful', 'wonderful', 'perfect',\n    'excellent', 'awesome', 'fantastic', 'incredible', 'lovely', 'stunning',\n    'gorgeous', 'breathtaking', 'magnificent', 'spectacular', 'delicious',\n    'tasty', 'pleasant', 'charming', 'picturesque', 'remarkable',\n    \n    # Négatifs français\n    'mauvais', 'nul', 'horrible', 'affreux', 'décevant', 'médiocre',\n    'terrible', 'catastrophique', 'désastreux', 'lamentable', 'sale',\n    \n    # Négatifs anglais\n    'bad', 'terrible', 'awful', 'horrible', 'disappointing', 'poor',\n    'worst', 'disgusting', 'unpleasant', 'dirty', 'ugly'\n}\n\ncustom_stopwords = {\n    'très', 'tout', 'tous', 'toute', 'toutes', 'vraiment', 'bien', 'beaucoup',\n    'plus', 'moins', 'encore', 'aussi', 'alors', 'donc', 'ainsi', 'cela',\n    'place', 'lieu', 'endroit', 'visit', 'visite', 'visited', 'visité',\n    'trip', 'voyage', 'travel', 'tourisme', 'tourism', 'tourist', 'touriste',\n    'time', 'temps', 'moment', 'day', 'jour', 'journée', 'week', 'semaine',\n    'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles', 'on',\n    'de', 'du', 'des', 'à', 'au', 'aux', 'par', 'pour', 'avec', 'sans',\n    'le', 'la', 'les', 'un', 'une', 'des', 'the', 'a', 'an',\n    'i', 'you', 'he', 'she', 'we', 'they', 'it', 'my', 'your'\n}\n\nstop_words_combined = (stop_words_fr | stop_words_en | custom_stopwords) - sentiment_words_to_keep\n\nprint(f\"✅ Stopwords : {len(stop_words_combined)} mots vides\")\nprint(f\"✅ Mots de sentiment gardés : {len(sentiment_words_to_keep)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:00:03.651845Z","iopub.execute_input":"2025-12-02T19:00:03.652896Z","iopub.status.idle":"2025-12-02T19:00:03.661290Z","shell.execute_reply.started":"2025-12-02T19:00:03.652854Z","shell.execute_reply":"2025-12-02T19:00:03.660553Z"}},"outputs":[{"name":"stdout","text":"✅ Stopwords : 386 mots vides\n✅ Mots de sentiment gardés : 69\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ---------------------------\n# 📌 spaCy FR\n# ---------------------------\nimport spacy\nnlp_fr = spacy.load(\"fr_core_news_md\")\n\ndef lemmatize_fr_spacy(text):\n    doc = nlp_fr(text)\n    return [token.lemma_ for token in doc]\n\n# ---------------------------\n# 📌 preprocess_text()\n# ---------------------------\ndef preprocess_text(text, language='unknown', for_analysis=True):\n    if not text or len(text.strip()) < 5:\n        return \"\"\n\n    # --- Nettoyage basique ---\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n    text = re.sub(r'\\S+@\\S+', '', text)\n    text = re.sub(r'@\\w+|#\\w+', '', text)\n    text = re.sub(r'\\b\\d+\\b', '', text)\n    text = re.sub(r'[^\\w\\s\\'àâäéèêëïîôùûüÿçñ-]', ' ', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    if not for_analysis:\n        return text\n\n    # --- Tokenisation ---\n    tokens = word_tokenize(text)\n\n    # --- Filtrage smart ---\n    tokens_filtered = []\n    for t in tokens:\n        if t in sentiment_words_to_keep:\n            tokens_filtered.append(t)\n        elif t not in stop_words_combined and len(t) > 2:\n            tokens_filtered.append(t)\n    tokens = tokens_filtered\n\n    # --- Lemmatisation selon langue ---\n    if language in ['english', 'en']:\n        tokens = [t if t in sentiment_words_to_keep else lemmatizer_en.lemmatize(t) for t in tokens]\n\n    elif language in ['french', 'fr']:\n        lemmas = lemmatize_fr_spacy(' '.join(tokens))\n        tokens = [t if t in sentiment_words_to_keep else l for t, l in zip(tokens, lemmas)]\n\n    # --- Final Clean ---\n    tokens = [t for t in tokens if len(t) > 2 or t in sentiment_words_to_keep]\n\n    return \" \".join(tokens)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:00:15.326140Z","iopub.execute_input":"2025-12-02T19:00:15.326713Z","iopub.status.idle":"2025-12-02T19:00:17.775246Z","shell.execute_reply.started":"2025-12-02T19:00:15.326686Z","shell.execute_reply":"2025-12-02T19:00:17.774585Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(\"\\n📂 Chargement du dataset...\")\ndf = pd.read_csv('/kaggle/input/tunisia-sensory-data/tunisia_natural_comments_10000_20251201_194300.csv')\nprint(f\"✅ Dataset chargé : {len(df)} commentaires\")\n\ndf['text'] = df['text'].fillna('').astype(str)\ndf['language'] = df['language'].fillna('unknown').astype(str).str.lower()\n\nprint(f\"\\n📊 Distribution des langues :\")\nprint(df['language'].value_counts())\n\n# Colonnes pour comparaison\ndf['text_before'] = df['text']\ndf['text_clean'] = df.apply(lambda row: preprocess_text(row['text'], row['language'], False), axis=1)\ndf['text_processed'] = df.apply(lambda row: preprocess_text(row['text'], row['language'], True), axis=1)\n\n# Filtrage minimal\ndf = df[df['text_clean'].str.len() > 20]\ndf = df[df['text_processed'].str.len() > 10]\n\nprint(f\"✅ Prétraitement terminé : {len(df)} commentaires valides\")\n\n\n# 🔍 EXEMPLES\nprint(\"\\n🔍 EXEMPLES NETTOYAGE :\")\nfor i in range(5):\n    print(\"\\n--- Exemple\", i+1)\n    print(\"📌 Original :\", df['text_before'].iloc[i])\n    print(\"🧽 Clean    :\", df['text_clean'].iloc[i])\n    print(\"🧠 Processé :\", df['text_processed'].iloc[i])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:00:19.767551Z","iopub.execute_input":"2025-12-02T19:00:19.768616Z","iopub.status.idle":"2025-12-02T19:01:31.524243Z","shell.execute_reply.started":"2025-12-02T19:00:19.768577Z","shell.execute_reply":"2025-12-02T19:01:31.523337Z"}},"outputs":[{"name":"stdout","text":"\n📂 Chargement du dataset...\n✅ Dataset chargé : 10000 commentaires\n\n📊 Distribution des langues :\nlanguage\nfr    6028\nen    3972\nName: count, dtype: int64\n✅ Prétraitement terminé : 9999 commentaires valides\n\n🔍 EXEMPLES NETTOYAGE :\n\n--- Exemple 1\n📌 Original : I have never been more happy on this trip because of the beautiful scenery we saw!\n🧽 Clean    : i have never been more happy on this trip because of the beautiful scenery we saw\n🧠 Processé : never happy beautiful scenery saw\n\n--- Exemple 2\n📌 Original : You can't go wrong with the Jazz Festival in your backyard if you have some good jazz music going for you! The next day after that visit to Togo took me back home again (the place is located 3 miles from where my trip started).\n🧽 Clean    : you can't go wrong with the jazz festival in your backyard if you have some good jazz music going for you the next day after that visit to togo took me back home again the place is located miles from where my trip started\n🧠 Processé : n't wrong jazz festival backyard good jazz music going next togo took back home located mile started\n\n--- Exemple 3\n📌 Original : Je viens de passer quelques jours à Sidi Bou Said et c'était fantastique. En tant que un couple en voyage de noces, j'ai vraiment apprécié Café des Nattouille-Jolieurs and je veux qui se défendre lui avec votres sont présentatiques; le plus pareil dans la tête est mais il faut suivant les établises sur être réputation au coursier du chacun orceau faire parcussée !\n🧽 Clean    : je viens de passer quelques jours à sidi bou said et c'était fantastique en tant que un couple en voyage de noces j'ai vraiment apprécié café des nattouille-jolieurs and je veux qui se défendre lui avec votres sont présentatiques le plus pareil dans la tête est mais il faut suivant les établises sur être réputation au coursier du chacun orceau faire parcussée\n🧠 Processé : venir passer quelque jour sidi bou said fantastique fantastique tant couple noce avoir apprécier café nattouille jolieur vouloir défendre votres présentatique pareil tête falloir suivre établise être réputation coursier\n\n--- Exemple 4\n📌 Original : It's just so cool when you get to see the world outside your walls and windows.\" Pressed by TheBlaze about how she feels now that her parents have returned home after nearly three years away, Youssef went through \"a series\" of difficult questions: What would it be like working in France? Will they live happily ever after if Tunisia goes along-with them?\n🧽 Clean    : it's just so cool when you get to see the world outside your walls and windows pressed by theblaze about how she feels now that her parents have returned home after nearly three years away youssef went through a series of difficult questions what would it be like working in france will they live happily ever after if tunisia goes along-with them\n🧠 Processé : cool get see world outside wall window pressed theblaze feel parent returned home nearly three year away youssef went series difficult question would like working france live happily ever tunisia along-with\n\n--- Exemple 5\n📌 Original : Je reviens d'un séjour à Carthage et je voulais partager mon expérience incroyable. En tant que un couple retraité qui voyage, j'ai vraiment apprécié une même nouvelle de la tournée en répondent les gens du place démontalmente and avec leurs travails sur lui percipitated pour sa premier des plumes était entre penseur; il faut ou président quelque est cela parquelle peut-il contenant plus commerse auquel lieu son restaient: \"Ah!\n🧽 Clean    : je reviens d'un séjour à carthage et je voulais partager mon expérience incroyable en tant que un couple retraité qui voyage j'ai vraiment apprécié une même nouvelle de la tournée en répondent les gens du place démontalmente and avec leurs travails sur lui percipitated pour sa premier des plumes était entre penseur il faut ou président quelque est cela parquelle peut-il contenant plus commerse auquel lieu son restaient ah\n🧠 Processé : revenir séjour carthage vouloir partager incroyable incroyable tant couple retraité avoir apprécier nouveau tournée répondre gens démontalmente leur travail percipitated premier plume entrer penseur falloir président quelque parquelle pouvoir contenir\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"print(\"\\n💚 Analyse de sentiment 3 classes avec calcul de confiance\")\n\nsentiment_classifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n    device=device,\n    batch_size=32\n)\n\nsentiment_labels = [\n    \"positive experience\",\n    \"neutral experience\",\n    \"negative experience\"\n]\n\ndef classify_sentiment_batch(texts, batch_size=32):\n    \"\"\"Classification sentiment avec calcul de confiance\"\"\"\n    results = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"🔍 Sentiment\"):\n        batch = texts[i:i+batch_size]\n        try:\n            batch_results = sentiment_classifier(batch, candidate_labels=sentiment_labels, multi_label=False)\n            for result in batch_results:\n                top_label = result['labels'][0]\n                top_score = result['scores'][0]\n                \n                # Mapping 3 classes\n                if \"positive\" in top_label:\n                    sentiment_class = 3\n                elif \"neutral\" in top_label:\n                    sentiment_class = 2\n                else:\n                    sentiment_class = 1\n                \n                # ✅ CALCUL DE LA CONFIANCE\n                # Confiance haute si score > 0.7, moyenne si 0.5-0.7, faible si < 0.5\n                if top_score > 0.7:\n                    confidence = \"high\"\n                elif top_score > 0.5:\n                    confidence = \"medium\"\n                else:\n                    confidence = \"low\"\n                \n                results.append({\n                    'sentiment_class': sentiment_class,\n                    'sentiment_score': top_score,\n                    'sentiment_confidence': confidence,\n                    'sentiment_label': top_label\n                })\n        except Exception as e:\n            print(f\"⚠️ Erreur batch {i}: {e}\")\n            results.extend([{\n                'sentiment_class': 2,\n                'sentiment_score': 0.5,\n                'sentiment_confidence': 'low',\n                'sentiment_label': 'neutral experience'\n            }] * len(batch))\n    return results\n\n# Application\nsentiment_results = classify_sentiment_batch(df['text_clean'].tolist())\ndf['sentiment_class'] = [r['sentiment_class'] for r in sentiment_results]\ndf['sentiment_score'] = [r['sentiment_score'] for r in sentiment_results]\ndf['sentiment_confidence'] = [r['sentiment_confidence'] for r in sentiment_results]\ndf['sentiment_label'] = [r['sentiment_label'] for r in sentiment_results]\n\nprint(f\"\\n✅ Distribution du sentiment :\")\nprint(df['sentiment_class'].value_counts().sort_index())\nprint(f\"\\n✅ Distribution de confiance :\")\nprint(df['sentiment_confidence'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:03:00.728075Z","iopub.execute_input":"2025-12-02T19:03:00.729043Z","iopub.status.idle":"2025-12-02T19:08:10.061047Z","shell.execute_reply.started":"2025-12-02T19:03:00.729012Z","shell.execute_reply":"2025-12-02T19:08:10.060010Z"}},"outputs":[{"name":"stdout","text":"\n💚 Analyse de sentiment 3 classes avec calcul de confiance\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"013197eef8ed4c4ba92f67e16bd9be0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9377fb244c84460bf5811d818af2c77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/467 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"786e36eefeac4e2388c6f11e3d2f9139"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2791ceab31c3405698e984703b215e45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/16.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194569ece1cc482887c6ac8b68439443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"822064d03ee94c5e8594713b14fc95d5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/173 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"816c754446e64b399ca46aeade100632"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"🔍 Sentiment:   0%|          | 0/313 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1818c4984d046a78c95ae7320f1f0ad"}},"metadata":{}},{"name":"stderr","text":"You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","output_type":"stream"},{"name":"stdout","text":"\n✅ Distribution du sentiment :\nsentiment_class\n1    1464\n2     601\n3    7934\nName: count, dtype: int64\n\n✅ Distribution de confiance :\nsentiment_confidence\nhigh      7696\nmedium    1660\nlow        643\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset\n\n# Mapper target_sentiment en labels numériques\nlabel_mapping = {\"negative\":0, \"neutral\":1, \"positive\":2}\ndf['label'] = df['target_sentiment'].map(label_mapping)\n\n# Supprimer les lignes avec target_sentiment manquant\ndf = df[df['label'].notna()]\n\n# Split train/test\ntrain_df, test_df = train_test_split(df[['text_clean','label']], test_size=0.2, random_state=42, stratify=df['label'])\n\n# Convertir en Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:27:27.980443Z","iopub.execute_input":"2025-12-02T20:27:27.980792Z","iopub.status.idle":"2025-12-02T20:27:28.042365Z","shell.execute_reply.started":"2025-12-02T20:27:27.980757Z","shell.execute_reply":"2025-12-02T20:27:28.041557Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Mapping des labels réels en nombres (même logique que sentiment_class)\nsentiment_mapping = {\n    \"positive\": 3,\n    \"neutral\": 2,\n    \"negative\": 1\n}\n\n# Supprimer les lignes avec target_sentiment manquant\ndf_clean = df[df['target_sentiment'].notna()]\n\n# Préparer y_true et y_pred\ny_true = df_clean['target_sentiment'].map(sentiment_mapping)\ny_pred = df_clean['sentiment_class']\n\n# Calcul des métriques\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\n\naccuracy = accuracy_score(y_true, y_pred)\nf1_macro = f1_score(y_true, y_pred, average='macro')\nf1_weighted = f1_score(y_true, y_pred, average='weighted')\n\nprint(f\"\\n📊 Accuracy: {accuracy:.4f}\")\nprint(f\"F1-score (macro): {f1_macro:.4f}\")\nprint(f\"F1-score (weighted): {f1_weighted:.4f}\")\n\nprint(\"\\n📄 Classification Report :\")\nprint(classification_report(y_true, y_pred, target_names=[\"negative\", \"neutral\", \"positive\"]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:22:50.927348Z","iopub.execute_input":"2025-12-02T20:22:50.927683Z","iopub.status.idle":"2025-12-02T20:22:50.961206Z","shell.execute_reply.started":"2025-12-02T20:22:50.927657Z","shell.execute_reply":"2025-12-02T20:22:50.960589Z"}},"outputs":[{"name":"stdout","text":"\n📊 Accuracy: 0.6704\nF1-score (macro): 0.4674\nF1-score (weighted): 0.6316\n\n📄 Classification Report :\n              precision    recall  f1-score   support\n\n    negative       0.62      0.44      0.52      2033\n     neutral       0.17      0.07      0.10      1490\n    positive       0.72      0.88      0.79      6476\n\n    accuracy                           0.67      9999\n   macro avg       0.50      0.46      0.47      9999\nweighted avg       0.62      0.67      0.63      9999\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from tqdm import tqdm\nfrom transformers import pipeline\n\n# 🔹 Pipeline zero-shot\nsense_classifier = pipeline(\n    \"zero-shot-classification\",\n    model=\"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\",\n    device=0  # ou -1 si pas de GPU\n)\n\n# 🔹 Labels pour le zero-shot (français + anglais pour meilleure précision\nsense_labels = {\n    'vue': \"ce texte décrit des expériences visuelles, ce qui peut être vu | visual experiences and what can be seen\",\n    'ouie': \"ce texte décrit des sons, de la musique et des expériences auditives | sounds, music and auditory experiences\",\n    'odorat': \"ce texte décrit des odeurs, parfums et expériences olfactives | smells, scents and olfactory experiences\",\n    'gout': \"ce texte décrit des goûts, saveurs et expériences alimentaires | tastes, flavors and food experiences\",\n    'toucher': \"ce texte décrit des sensations tactiles et physiques | tactile sensations, textures and physical touch\"\n}\n\n# 🔹 Dictionnaire de mots-clés sensoriels enrichi (français + anglais)\nsense_dict = {\n    'vue': ['lumière', 'couleur', 'paysage', 'photo', 'vidéo', 'image', 'scène', 'visual', 'color', 'sight', 'view', 'landscape', 'panorama', 'horizon', 'éclat', 'brillance', 'ombre', 'contraste', 'forme', 'silhouette', 'reflet', 'miroir', 'lueur', 'clarté', 'obscurité', 'teinte', 'nuance', 'saturation', 'luminosité', 'perspective', 'profondeur', 'détail', 'flou', 'netteté', 'beauté', 'magnifique', 'éblouissant', 'sombre', 'clair', 'vif', 'pastel', 'néon', 'arc-en-ciel', 'aurore', 'crépuscule', 'étoile', 'ciel', 'mer', 'montagne', 'forêt', 'désert', 'ville', 'flash', 'scintillement', 'kaléidoscope'],\n\n    'ouie': ['musique', 'bruit', 'son', 'voix', 'chant', 'battement', 'auditory', 'sound', 'voice', 'music', 'noise', 'mélodie', 'rythme', 'harmonie', 'symphonie', 'concert', 'guitare', 'piano', 'batterie', 'violon', 'jazz', 'rock', 'classique', 'pop', 'rap', 'hip-hop', 'reggae', 'techno', 'électro', 'metal', 'blues', 'folk', 'opéra', 'chœur', 'solo', 'accord', 'note', 'silence', 'écho', 'résonance', 'vibration', 'tonnerre', 'pluie', 'vent', 'vagues', 'cri', 'rire', 'pleur', 'murmure', 'chuchotement', 'hurlement', 'klaxon', 'sirène', 'cloche', 'tic-tac', 'explosion', 'feuillage', 'pas', 'cœur', 'souffle'],\n\n    'odorat': ['odeur', 'parfum', 'arôme', 'fumée', 'senteur', 'smell', 'scent', 'fragrance', 'aroma', 'odor', 'fleur', 'rose', 'jasmin', 'lavande', 'vanille', 'chocolat', 'café', 'pain', 'gâteau', 'barbecue', 'grillé', 'poisson', 'fromage', 'vin', 'herbe', 'terre', 'pluie', 'forêt', 'pin', 'eucalyptus', 'menthe', 'citron', 'orange', 'épices', 'cannelle', 'musc', 'sueur', 'cuir', 'bois', 'encens', 'bougie', 'essence', 'pétrole', 'mer', 'algue', 'moisi', 'pourri', 'frais', 'doux', 'entêtant', 'subtil', 'âcre', 'sucré', 'boisé', 'fruité', 'floral'],\n\n    'gout': ['goût', 'saveur', 'sucré', 'salé', 'amer', 'piquant', 'acide', 'umami', 'food', 'taste', 'flavor', 'delicious', 'sweet', 'salty', 'bitter', 'spicy', 'sour', 'délicieux', 'bon', 'exquis', 'savoureux', 'fondant', 'croustillant', 'juteux', 'crémeux', 'chocolaté', 'vanillé', 'fruité', 'épicé', 'poivré', 'ailé', 'citronné', 'sucré-salé', 'aigre-doux', 'brûlant', 'rafraîchissant', 'riche', 'léger', 'corsé', 'subtil', 'intense', 'gourmand', 'appétissant', 'fade', 'relevé', 'parfumé', 'onctueux', 'croquant', 'moelleux'],\n\n    'toucher': ['doux', 'rugueux', 'chaud', 'froid', 'texture', 'touch', 'soft', 'rough', 'smooth', 'tactile', 'cold', 'warm', 'soyeux', 'velours', 'coton', 'laine', 'cuir', 'métal', 'bois', 'pierre', 'sable', 'eau', 'mousse', 'épineux', 'piquant', 'glissant', 'collant', 'visqueux', 'granuleux', 'lisse', 'rêche', 'duveteux', 'hérissé', 'caressant', 'brûlant', 'glacial', 'humide', 'sec', 'poisseux', 'élastique', 'ferme', 'mou', 'dur', 'fragile', 'cassant', 'lourd', 'léger', 'vibrant', 'chatouille', 'picotement', 'pression', 'caresse', 'frisson']\n}\n\n# 🔹 Fonction de détection hybride (zero-shot + dictionnaire)\ndef detect_senses_hybrid(texts, batch_size=32, threshold=0.25):\n    all_senses = []\n\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Détection des 5 sens (hybride)\"):\n        batch = texts[i:i + batch_size]\n\n        # --- Zero-shot classification\n        try:\n            results = sense_classifier(\n                batch,\n                candidate_labels=list(sense_labels.values()),\n                multi_label=True\n            )\n        except Exception as e:\n            print(f\"Erreur zero-shot sur le batch {i//batch_size}: {e}\")\n            results = [{\"labels\": [], \"scores\": []} for _ in batch]\n\n        # --- Traitement du batch\n        for idx, text in enumerate(batch):\n            detected = {}\n\n            # Zero-shot scores\n            for label, score in zip(results[idx][\"labels\"], results[idx][\"scores\"]):\n                if score >= threshold:\n                    sense_name = [k for k, v in sense_labels.items() if v == label][0]\n                    detected[sense_name] = max(detected.get(sense_name, 0.0), score)\n\n            # Boost avec dictionnaire de mots-clés\n            text_lower = text.lower()\n            for sense, keywords in sense_dict.items():\n                if any(kw.lower() in text_lower for kw in keywords):\n                    # On donne un score élevé si mot-clé trouvé (priorité au dictionnaire)\n                    detected[sense] = max(detected.get(sense, 0.0), 0.85)\n\n            all_senses.append(detected)\n\n    return all_senses\n\n# 🔹 Application sur ton DataFrame\nprint(\"\\nDétection des 5 sens en cours...\")\nsenses_results = detect_senses_hybrid(df['text_clean'].tolist(), batch_size=16)  # 16 recommandé sur GPU\n\ndf['detected_senses'] = senses_results\n\n# Création des colonnes binaires + scores\nfor sense in ['vue', 'ouie', 'odorat', 'gout', 'toucher']:\n    df[f'has_{sense}'] = df['detected_senses'].apply(lambda x: 1 if sense in x and x[sense] > 0 else 0)\n    df[f'{sense}_score'] = df['detected_senses'].apply(lambda x: x.get(sense, 0.0))\n\n# 🔹 Résultats\nprint(\"\\nExemples :\")\nprint(df[['text_clean', 'detected_senses', 'has_vue', 'has_ouie', 'has_odorat', 'has_gout', 'has_toucher']].head(10))\n\nprint(\"\\nStatistiques globales :\")\nfor sense in ['vue', 'ouie', 'odorat', 'gout', 'toucher']:\n    print(f\"{sense.capitalize():8} → {df[f'has_{sense}'].sum()} textes ({df[f'has_{sense}'].mean():.1%})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T19:54:44.236724Z","iopub.execute_input":"2025-12-02T19:54:44.237552Z","iopub.status.idle":"2025-12-02T20:17:52.011278Z","shell.execute_reply.started":"2025-12-02T19:54:44.237527Z","shell.execute_reply":"2025-12-02T20:17:52.010157Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nDétection des 5 sens en cours...\n","output_type":"stream"},{"name":"stderr","text":"Détection des 5 sens (hybride): 100%|██████████| 625/625 [23:04<00:00,  2.22s/it]","output_type":"stream"},{"name":"stdout","text":"\nExemples :\n                                          text_clean  \\\n0  i have never been more happy on this trip beca...   \n1  you can't go wrong with the jazz festival in y...   \n2  je viens de passer quelques jours à sidi bou s...   \n3  it's just so cool when you get to see the worl...   \n4  je reviens d'un séjour à carthage et je voulai...   \n5  the only thing that surprised me is how well t...   \n6  je dois être honnête sur mon expérience à sidi...   \n7  it's cool to know what you have around here is...   \n8  mon voyage à hammamet a été absolument merveil...   \n9  i just don't think this is enough for me at th...   \n\n                                     detected_senses  has_vue  has_ouie  \\\n0  {'vue': 0.9920822381973267, 'gout': 0.92671632...        1         1   \n1  {'vue': 0.9061977863311768, 'toucher': 0.49154...        1         1   \n2  {'toucher': 0.9769518971443176, 'vue': 0.97554...        1         1   \n3  {'vue': 0.9677971601486206, 'odorat': 0.899123...        1         1   \n4  {'toucher': 0.9603913426399231, 'odorat': 0.95...        1         1   \n5  {'vue': 0.7849014401435852, 'odorat': 0.85, 't...        1         1   \n6  {'vue': 0.9418572783470154, 'odorat': 0.913271...        1         1   \n7  {'vue': 0.9634872078895569, 'odorat': 0.844322...        1         1   \n8  {'ouie': 0.979725182056427, 'vue': 0.936691403...        1         1   \n9  {'vue': 0.9892834424972534, 'odorat': 0.975275...        1         1   \n\n   has_odorat  has_gout  has_toucher  \n0           1         1            1  \n1           1         0            1  \n2           1         1            1  \n3           1         1            1  \n4           1         1            1  \n5           1         1            1  \n6           1         1            1  \n7           1         1            1  \n8           1         1            1  \n9           1         1            1  \n\nStatistiques globales :\nVue      → 9969 textes (99.7%)\nOuie     → 9464 textes (94.6%)\nOdorat   → 9803 textes (98.0%)\nGout     → 8434 textes (84.3%)\nToucher  → 9935 textes (99.4%)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"\\n🎭 Extraction des mascottes sensorielles...\")\n\ndef extract_mascot_for_sense(texts_processed, texts_clean, sense_name, top_n=5):\n    \"\"\"\n    Extraction de la mascotte (mot-clé le plus représentatif) pour un sens donné\n    \n    Args:\n        texts_processed: Textes tokenisés pour analyse de fréquence\n        texts_clean: Textes nettoyés pour contexte\n        sense_name: Nom du sens (vue, ouie, etc.)\n        top_n: Nombre de candidats à considérer\n    \n    Returns:\n        str: Mascotte du sens ou \"non détecté\"\n    \"\"\"\n    if len(texts_processed) < 3:\n        return \"non détecté\"\n    \n    # Mots-clés liés au sens (pour filtrer)\n    sense_keywords = {\n        'vue': {'couleur', 'lumière', 'paysage', 'architecture', 'vue', 'color', 'light', 'view', 'landscape', 'beautiful'},\n        'ouie': {'musique', 'son', 'bruit', 'silence', 'chanson', 'music', 'sound', 'noise', 'song', 'quiet'},\n        'odorat': {'odeur', 'parfum', 'senteur', 'arôme', 'smell', 'scent', 'fragrance', 'aroma', 'perfume'},\n        'gout': {'goût', 'saveur', 'délicieux', 'plat', 'cuisine', 'taste', 'flavor', 'delicious', 'food', 'dish'},\n        'toucher': {'texture', 'sensation', 'doux', 'rugueux', 'température', 'texture', 'feeling', 'soft', 'rough', 'warm'}\n    }\n    \n    # Comptage des mots\n    all_words = []\n    for text in texts_processed:\n        words = text.split()\n        all_words.extend(words)\n    \n    # Fréquence des mots\n    word_freq = Counter(all_words)\n    \n    # Filtrer pour garder les mots pertinents au sens\n    relevant_words = []\n    for word, freq in word_freq.most_common(50):\n        if len(word) > 3 and freq >= 2:\n            # Prioriser les mots liés au sens\n            if word in sense_keywords.get(sense_name, set()):\n                relevant_words.append((word, freq * 2))  # Boost\n            else:\n                relevant_words.append((word, freq))\n    \n    # Trier par fréquence\n    relevant_words.sort(key=lambda x: x[1], reverse=True)\n    \n    if relevant_words:\n        mascot = relevant_words[0][0]\n        # Vérifier dans les textes originaux pour validation\n        context_count = sum(1 for text in texts_clean if mascot in text.lower())\n        if context_count >= 2:\n            return mascot\n    \n    return \"non détecté\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:33:53.795234Z","iopub.execute_input":"2025-12-02T20:33:53.795798Z","iopub.status.idle":"2025-12-02T20:33:53.805551Z","shell.execute_reply.started":"2025-12-02T20:33:53.795770Z","shell.execute_reply":"2025-12-02T20:33:53.804697Z"}},"outputs":[{"name":"stdout","text":"\n🎭 Extraction des mascottes sensorielles...\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(\"\\n🗺️ CRÉATION DE LA CARTE SENSORIELLE PAR LIEU - VERSION OPTIMISÉE\")\nprint(\"=\"*80)\n\n# Filtrer les lieux avec au moins 5 commentaires\nplaces_count = df['place'].value_counts()\nvalid_places = places_count[places_count >= 5].index\ndf_valid = df[df['place'].isin(valid_places)].copy()\n\n# ============================================================================ #\n# 1️⃣ Positivité globale par lieu\n# ============================================================================ #\n# Créer colonne binaire pour positivité globale\ndf_valid['is_positive'] = df_valid['sentiment_class'] == 3\n\n# Groupby vectorisé pour la positivité globale\nglobal_stats = df_valid.groupby('place').agg(\n    total_comments=('text', 'count'),\n    positive_comments=('is_positive', 'sum')\n).reset_index()\n\nglobal_stats['positivite_globale_%'] = (\n    global_stats['positive_comments'] / global_stats['total_comments'] * 100\n).round(2)\n\n# ============================================================================ #\n# 2️⃣ Positivité par sens (VECTORISÉ)\n# ============================================================================ #\nsenses = ['vue', 'ouie', 'odorat', 'gout', 'toucher']\n\nfor sense in senses:\n    has_col = f'has_{sense}'\n    # Binaire + positivité\n    df_valid[f'{sense}_positive'] = df_valid[has_col] & df_valid['is_positive']\n    \n    # Groupby vectorisé\n    sense_stats = df_valid.groupby('place').agg(\n        total_comments=(has_col, 'sum'),\n        positive_comments=(f'{sense}_positive', 'sum')\n    ).rename(columns={\n        'total_comments': f'{sense}_total_comments',\n        'positive_comments': f'{sense}_positive_comments'\n    })\n    \n    # % positivité\n    sense_stats[f'{sense}_positivite_%'] = (\n        (sense_stats[f'{sense}_positive_comments'] / sense_stats[f'{sense}_total_comments'])\n        .fillna(0) * 100\n    ).round(2)\n    \n    # Fusion avec global_stats\n    global_stats = global_stats.merge(sense_stats, left_on='place', right_index=True, how='left')\n\n# Remplacer NaN par 0 pour les sens non mentionnés\nglobal_stats.fillna(0, inplace=True)\n\n# ============================================================================ #\n# 3️⃣ Extraction des mascottes par sens\n# ============================================================================ #\nprint(\"\\n🎭 Extraction des mascottes sensorielles...\")\n\nfor sense in tqdm(senses, desc=\"🎭 Mascottes\"):\n    mascots = {}\n    for place in df_valid['place'].unique():\n        place_df = df_valid[df_valid['place'] == place]\n        sense_positive = place_df[place_df[f'{sense}_positive'] == True]\n        \n        if len(sense_positive) >= 3:\n            mascot = extract_mascot_for_sense(\n                sense_positive['text_processed'].tolist(),\n                sense_positive['text_clean'].tolist(),\n                sense\n            )\n        else:\n            mascot = \"non détecté\"\n        \n        mascots[place] = mascot\n    \n    # Ajouter colonne mascotte\n    global_stats[f'{sense}_mascotte'] = global_stats['place'].map(mascots)\n\n# Trier par positivité globale\nsensory_map_df = global_stats.sort_values('positivite_globale_%', ascending=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:33:58.180503Z","iopub.execute_input":"2025-12-02T20:33:58.181062Z","iopub.status.idle":"2025-12-02T20:33:58.616884Z","shell.execute_reply.started":"2025-12-02T20:33:58.181033Z","shell.execute_reply":"2025-12-02T20:33:58.616171Z"}},"outputs":[{"name":"stdout","text":"\n🗺️ CRÉATION DE LA CARTE SENSORIELLE PAR LIEU - VERSION OPTIMISÉE\n================================================================================\n\n🎭 Extraction des mascottes sensorielles...\n","output_type":"stream"},{"name":"stderr","text":"🎭 Mascottes: 100%|██████████| 5/5 [00:00<00:00, 13.54it/s]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(\"\\n💾 Sauvegarde des résultats...\")\n\nsensory_map_df.to_csv('TUNISIA_SENSORY_MAP_FINAL_FIXED.csv', index=False, encoding='utf-8-sig')\nwith open('TUNISIA_SENSORY_MAP_FINAL_FIXED.json', 'w', encoding='utf-8') as f:\n    json.dump(sensory_map_df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n\nprint(\"\\n✅ ANALYSE TERMINÉE !\")\nprint(f\"\\n🏆 TOP 10 LIEUX PAR POSITIVITÉ :\\n\")\nfor idx, row in sensory_map_df.head(10).iterrows():\n    print(f\"\\n📍 {row['place']} ({int(row['total_comments'])} commentaires)\")\n    print(f\"   💚 Positivité : {row['positivite_globale_%']:.1f}%\")\n    print(f\"   👁️  Vue: {row.get('vue_mascotte', 'N/A')}\")\n    print(f\"   👂 Ouïe: {row.get('ouie_mascotte', 'N/A')}\")\n    print(f\"   👃 Odorat: {row.get('odorat_mascotte', 'N/A')}\")\n    print(f\"   👅 Goût: {row.get('gout_mascotte', 'N/A')}\")\n    print(f\"   ✋ Toucher: {row.get('toucher_mascotte', 'N/A')}\")\n\nprint(\"\\n📦 Fichiers générés :\")\nprint(\"   ✅ TUNISIA_SENSORY_MAP_FINAL_FIXED.csv\")\nprint(\"   ✅ TUNISIA_SENSORY_MAP_FINAL_FIXED.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:34:04.170640Z","iopub.execute_input":"2025-12-02T20:34:04.171342Z","iopub.status.idle":"2025-12-02T20:34:04.194978Z","shell.execute_reply.started":"2025-12-02T20:34:04.171314Z","shell.execute_reply":"2025-12-02T20:34:04.194162Z"}},"outputs":[{"name":"stdout","text":"\n💾 Sauvegarde des résultats...\n\n✅ ANALYSE TERMINÉE !\n\n🏆 TOP 10 LIEUX PAR POSITIVITÉ :\n\n\n📍 Sidi Bou Said (2036 commentaires)\n   💚 Positivité : 80.5%\n   👁️  Vue: said\n   👂 Ouïe: said\n   👃 Odorat: said\n   👅 Goût: said\n   ✋ Toucher: said\n\n📍 Djerba (1983 commentaires)\n   💚 Positivité : 80.2%\n   👁️  Vue: avoir\n   👂 Ouïe: avoir\n   👃 Odorat: avoir\n   👅 Goût: avoir\n   ✋ Toucher: avoir\n\n📍 Hammamet (2063 commentaires)\n   💚 Positivité : 80.0%\n   👁️  Vue: avoir\n   👂 Ouïe: avoir\n   👃 Odorat: avoir\n   👅 Goût: avoir\n   ✋ Toucher: avoir\n\n📍 Tunis Medina (1956 commentaires)\n   💚 Positivité : 78.7%\n   👁️  Vue: avoir\n   👂 Ouïe: avoir\n   👃 Odorat: avoir\n   👅 Goût: avoir\n   ✋ Toucher: avoir\n\n📍 Carthage (1961 commentaires)\n   💚 Positivité : 77.3%\n   👁️  Vue: avoir\n   👂 Ouïe: avoir\n   👃 Odorat: avoir\n   👅 Goût: avoir\n   ✋ Toucher: avoir\n\n📦 Fichiers générés :\n   ✅ TUNISIA_SENSORY_MAP_FINAL_FIXED.csv\n   ✅ TUNISIA_SENSORY_MAP_FINAL_FIXED.json\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"# CELLULE 9 OPTIMISÉE : CARTE SENSORIELLE PAR LIEU (VECTORISÉE)\n################################################################################\n# Analyse détaillée des lieux avec calcul vectorisé pour performance\n################################################################################\n\nprint(\"🗺️ CRÉATION DE LA CARTE SENSORIELLE PAR LIEU - VERSION OPTIMISÉE\")\nprint(\"=\"*80)\n\n# Filtrer les lieux avec au moins 5 commentaires\nplaces_count = df['place'].value_counts()\nvalid_places = places_count[places_count >= 5].index\ndf_valid = df[df['place'].isin(valid_places)].copy()\n\n# ============================================================================ #\n# 1️⃣ Positivité globale par lieu\n# ============================================================================ #\n# Créer colonne binaire pour positivité globale\ndf_valid['is_positive'] = df_valid['sentiment_class'] == 3\n\n# Groupby vectorisé pour la positivité globale\nglobal_stats = df_valid.groupby('place').agg(\n    total_comments=('text', 'count'),\n    positive_comments=('is_positive', 'sum')\n).reset_index()\n\nglobal_stats['positivite_globale_%'] = (global_stats['positive_comments'] / global_stats['total_comments'] * 100).round(2)\n\n# ============================================================================ #\n# 2️⃣ Positivité par sens\n# ============================================================================ #\nsenses = ['vue', 'ouie', 'odorat', 'gout', 'toucher']\n\nfor sense in senses:\n    has_col = f'has_{sense}'\n    # Binaire + positivité\n    df_valid[f'{sense}_positive'] = df_valid[has_col] & df_valid['is_positive']\n    \n    # Groupby vectorisé\n    sense_stats = df_valid.groupby('place').agg(\n        total_comments=(has_col, 'sum'),\n        positive_comments=(f'{sense}_positive', 'sum')\n    ).rename(columns={\n        'total_comments': f'{sense}_total_comments',\n        'positive_comments': f'{sense}_positive_comments'\n    })\n    \n    # % positivité\n    sense_stats[f'{sense}_positivite_%'] = (\n        (sense_stats[f'{sense}_positive_comments'] / sense_stats[f'{sense}_total_comments'])\n        .fillna(0) * 100\n    ).round(2)\n    \n    # Fusion avec global_stats\n    global_stats = global_stats.merge(sense_stats, left_on='place', right_index=True, how='left')\n\n# Remplacer NaN par 0 pour les sens non mentionnés\nglobal_stats.fillna(0, inplace=True)\n\n# Trier par positivité globale\nsensory_map_detailed_df = global_stats.sort_values('positivite_globale_%', ascending=False)\n\n# ============================================================================ #\n# 3️⃣ Sauvegarde\n# ============================================================================ #\nsensory_map_detailed_df.to_csv('TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.csv', index=False, encoding='utf-8-sig')\nwith open('TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.json', 'w', encoding='utf-8') as f:\n    json.dump(sensory_map_detailed_df.to_dict(orient='records'), f, ensure_ascii=False, indent=2)\n\nprint(f\"\\n✅ Analyse vectorisée terminée : {len(sensory_map_detailed_df)} lieux\")\nprint(\"💾 Fichiers générés :\")\nprint(\"   📄 TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.csv\")\nprint(\"   📄 TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:34:14.723330Z","iopub.execute_input":"2025-12-02T20:34:14.723987Z","iopub.status.idle":"2025-12-02T20:34:14.789893Z","shell.execute_reply.started":"2025-12-02T20:34:14.723962Z","shell.execute_reply":"2025-12-02T20:34:14.789163Z"}},"outputs":[{"name":"stdout","text":"🗺️ CRÉATION DE LA CARTE SENSORIELLE PAR LIEU - VERSION OPTIMISÉE\n================================================================================\n\n✅ Analyse vectorisée terminée : 5 lieux\n💾 Fichiers générés :\n   📄 TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.csv\n   📄 TUNISIA_SENSORY_DETAILED_BY_PLACE_VECTOR.json\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Top 20 lieux par positivité globale\ntop20_places = sensory_map_detailed_df.head(20)\n\n# Affichage tabulaire\ntop20_places_display = top20_places[['place', 'total_comments', 'positive_comments', 'positivite_globale_%'] + \n                                    [f'{s}_positivite_%' for s in senses]]\ntop20_places_display\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T18:46:24.104488Z","iopub.status.idle":"2025-12-02T18:46:24.104682Z","shell.execute_reply.started":"2025-12-02T18:46:24.104586Z","shell.execute_reply":"2025-12-02T18:46:24.104594Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELLULE 10 : CARTE INTERACTIVE FOLIUM - CARTE SENSORIELLE DE LA TUNISIE (VERSION OPTIMISÉE)\n################################################################################\n# Visualisation interactive des lieux avec positivité par sens\n################################################################################\n\nimport folium\nfrom folium import plugins\nimport branca.colormap as cm\n\nprint(\"🗺️ CRÉATION DE LA CARTE INTERACTIVE FOLIUM\")\nprint(\"=\"*80)\n\n# Créer une copie du DataFrame optimisé pour la carte\nmap_data = sensory_map_detailed_df.copy()\n\n# Dictionnaire des coordonnées GPS\nTUNISIA_LOCATIONS = {\n    'Tunis': (36.8065, 10.1815),\n    'Carthage': (36.8531, 10.3231),\n    'Sidi Bou Said': (36.8689, 10.3411),\n    'La Marsa': (36.8781, 10.3247),\n    'Hammamet': (36.4000, 10.6167),\n    'Sousse': (35.8256, 10.6369),\n    'Monastir': (35.7772, 10.8261),\n    'Kairouan': (35.6781, 10.0967),\n    'Djerba': (33.8076, 10.8451),\n    # Ajoute tous les autres lieux nécessaires\n}\n\ndef get_coordinates(place_name):\n    place_clean = place_name.strip().lower()\n    for location, coords in TUNISIA_LOCATIONS.items():\n        if location.lower() == place_clean:\n            return coords\n    for location, coords in TUNISIA_LOCATIONS.items():\n        if location.lower() in place_clean or place_clean in location.lower():\n            return coords\n    return (34.0, 9.0)  # par défaut\n\n# Ajouter latitude et longitude\nmap_data['latitude'] = map_data['place'].apply(lambda x: get_coordinates(x)[0])\nmap_data['longitude'] = map_data['place'].apply(lambda x: get_coordinates(x)[1])\n\n# Carte centrée sur la Tunisie\ntunisia_map = folium.Map(location=[34.5, 9.5], zoom_start=7, tiles='OpenStreetMap', control_scale=True)\n\n# Colormap pour positivité globale\ncolormap = cm.LinearColormap(colors=['#d73027', '#fee08b', '#1a9850'], vmin=0, vmax=100, caption='Positivité Globale (%)')\ncolormap.add_to(tunisia_map)\n\nemoji_sens = {'vue': '👁️', 'ouie': '👂', 'odorat': '👃', 'gout': '👅', 'toucher': '✋'}\n\n# Ajout des marqueurs\nfor idx, row in map_data.iterrows():\n    color = colormap(row['positivite_globale_%'])\n    radius = min(5 + row['total_comments'] / 50, 20)\n    \n    popup_html = f\"\"\"\n    <div style=\"font-family: Arial; width: 350px;\">\n        <h3>📍 {row['place']}</h3>\n        <b>📊 {row['total_comments']} commentaires</b><br>\n        <b>💚 {row['positivite_globale_%']:.1f}% positifs</b><br>\n        <h4>🎭 Positivité par sens</h4>\n        <div><b>{emoji_sens['vue']} Vue</b>: {row['vue_positivite_%']:.0f}%</div>\n        <div><b>{emoji_sens['ouie']} Ouïe</b>: {row['ouie_positivite_%']:.0f}%</div>\n        <div><b>{emoji_sens['odorat']} Odorat</b>: {row['odorat_positivite_%']:.0f}%</div>\n        <div><b>{emoji_sens['gout']} Goût</b>: {row['gout_positivite_%']:.0f}%</div>\n        <div><b>{emoji_sens['toucher']} Toucher</b>: {row['toucher_positivite_%']:.0f}%</div>\n    </div>\n    \"\"\"\n\n    folium.CircleMarker(\n        location=[row['latitude'], row['longitude']],\n        radius=radius,\n        popup=folium.Popup(popup_html, max_width=400),\n        tooltip=f\"{row['place']} - {row['positivite_globale_%']:.1f}% positif\",\n        color='white',\n        fill=True,\n        fillColor=color,\n        fillOpacity=0.7,\n        weight=2\n    ).add_to(tunisia_map)\n\n# Plugins interactifs\nplugins.Search(layer=folium.FeatureGroup(name='Lieux'), geom_type='Point', placeholder='Rechercher un lieu...', collapsed=False).add_to(tunisia_map)\nplugins.LocateControl().add_to(tunisia_map)\nplugins.MeasureControl().add_to(tunisia_map)\nplugins.Fullscreen().add_to(tunisia_map)\nplugins.MiniMap(toggle_display=True).add_to(tunisia_map)\n\n# Heatmap\nheat_data = [[row['latitude'], row['longitude'], row['positivite_globale_%']/100] for idx, row in map_data.iterrows()]\nheatmap_layer = plugins.HeatMap(\n    heat_data,\n    name='Heatmap Positivité',\n    min_opacity=0.2,\n    max_zoom=13,\n    radius=25,\n    blur=35,\n    gradient={0.0:'blue',0.5:'yellow',1.0:'red'}\n)\ntunisia_map.add_child(heatmap_layer)\n\n# Cluster\nmarker_cluster = plugins.MarkerCluster(name='Clusters de lieux').add_to(tunisia_map)\nfor idx, row in map_data.iterrows():\n    folium.Marker(\n        location=[row['latitude'], row['longitude']],\n        popup=f\"{row['place']} - {row['positivite_globale_%']:.1f}% positif\"\n    ).add_to(marker_cluster)\n\n# Contrôle des couches\nfolium.LayerControl().add_to(tunisia_map)\n\n# Sauvegarde de la carte interactive\ntunisia_map.save('TUNISIA_SENSORY_MAP_INTERACTIVE.html')\nprint(\"\\n✅ Carte interactive créée avec succès !\")\nprint(\"📁 Fichier généré : TUNISIA_SENSORY_MAP_INTERACTIVE.html\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T20:38:04.751062Z","iopub.execute_input":"2025-12-02T20:38:04.751470Z","iopub.status.idle":"2025-12-02T20:38:04.808547Z","shell.execute_reply.started":"2025-12-02T20:38:04.751445Z","shell.execute_reply":"2025-12-02T20:38:04.807817Z"}},"outputs":[{"name":"stdout","text":"🗺️ CRÉATION DE LA CARTE INTERACTIVE FOLIUM\n================================================================================\n\n✅ Carte interactive créée avec succès !\n📁 Fichier généré : TUNISIA_SENSORY_MAP_INTERACTIVE.html\n","output_type":"stream"}],"execution_count":36}]}